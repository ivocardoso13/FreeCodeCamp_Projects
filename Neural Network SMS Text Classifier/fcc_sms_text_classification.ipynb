{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8RZOuS9LWQvv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-datasets\n",
            "  Using cached tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: absl-py in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (2.1.0)\n",
            "Requirement already satisfied: click in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (8.1.7)\n",
            "Collecting dm-tree (from tensorflow-datasets)\n",
            "  Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
            "Collecting immutabledict (from tensorflow-datasets)\n",
            "  Using cached immutabledict-4.2.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (1.26.4)\n",
            "Collecting promise (from tensorflow-datasets)\n",
            "  Using cached promise-2.3.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: protobuf>=3.20 in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (4.25.5)\n",
            "Requirement already satisfied: psutil in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (5.9.8)\n",
            "Collecting pyarrow (from tensorflow-datasets)\n",
            "  Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (2.32.3)\n",
            "Collecting simple-parsing (from tensorflow-datasets)\n",
            "  Using cached simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
            "  Using cached tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: termcolor in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (2.5.0)\n",
            "Collecting toml (from tensorflow-datasets)\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (4.66.5)\n",
            "Requirement already satisfied: wrapt in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets) (1.16.0)\n",
            "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
            "  Using cached etils-1.11.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting fsspec (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
            "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: typing_extensions in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.12.2)\n",
            "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
            "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.6.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->tensorflow-datasets) (0.4.6)\n",
            "Requirement already satisfied: six in c:\\users\\icardoso\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow-datasets)\n",
            "  Using cached docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
            "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Using cached tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
            "Using cached etils-1.11.0-py3-none-any.whl (165 kB)\n",
            "Downloading dm_tree-0.1.8-cp311-cp311-win_amd64.whl (101 kB)\n",
            "Using cached immutabledict-4.2.1-py3-none-any.whl (4.7 kB)\n",
            "Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
            "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.3/25.1 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.8/25.1 MB 1.8 MB/s eta 0:00:14\n",
            "   - -------------------------------------- 1.0/25.1 MB 1.6 MB/s eta 0:00:16\n",
            "   -- ------------------------------------- 1.3/25.1 MB 1.5 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 1.6/25.1 MB 1.4 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 1.6/25.1 MB 1.4 MB/s eta 0:00:17\n",
            "   -- ------------------------------------- 1.8/25.1 MB 1.2 MB/s eta 0:00:19\n",
            "   -- ------------------------------------- 1.8/25.1 MB 1.2 MB/s eta 0:00:19\n",
            "   --- ------------------------------------ 2.1/25.1 MB 1.1 MB/s eta 0:00:21\n",
            "   --- ------------------------------------ 2.4/25.1 MB 1.1 MB/s eta 0:00:22\n",
            "   --- ------------------------------------ 2.4/25.1 MB 1.1 MB/s eta 0:00:22\n",
            "   ---- ----------------------------------- 2.6/25.1 MB 1.0 MB/s eta 0:00:22\n",
            "   ---- ----------------------------------- 2.9/25.1 MB 1.0 MB/s eta 0:00:22\n",
            "   ---- ----------------------------------- 2.9/25.1 MB 1.0 MB/s eta 0:00:22\n",
            "   ----- ---------------------------------- 3.1/25.1 MB 1.0 MB/s eta 0:00:22\n",
            "   ----- ---------------------------------- 3.4/25.1 MB 1.0 MB/s eta 0:00:22\n",
            "   ----- ---------------------------------- 3.7/25.1 MB 995.9 kB/s eta 0:00:22\n",
            "   ----- ---------------------------------- 3.7/25.1 MB 995.9 kB/s eta 0:00:22\n",
            "   ------ --------------------------------- 3.9/25.1 MB 995.1 kB/s eta 0:00:22\n",
            "   ------ --------------------------------- 4.2/25.1 MB 983.0 kB/s eta 0:00:22\n",
            "   ------- -------------------------------- 4.5/25.1 MB 990.4 kB/s eta 0:00:21\n",
            "   ------- -------------------------------- 4.5/25.1 MB 990.4 kB/s eta 0:00:21\n",
            "   ------- -------------------------------- 4.7/25.1 MB 966.7 kB/s eta 0:00:22\n",
            "   ------- -------------------------------- 4.7/25.1 MB 966.7 kB/s eta 0:00:22\n",
            "   ------- -------------------------------- 5.0/25.1 MB 961.8 kB/s eta 0:00:21\n",
            "   -------- ------------------------------- 5.2/25.1 MB 960.0 kB/s eta 0:00:21\n",
            "   -------- ------------------------------- 5.2/25.1 MB 960.0 kB/s eta 0:00:21\n",
            "   -------- ------------------------------- 5.2/25.1 MB 960.0 kB/s eta 0:00:21\n",
            "   -------- ------------------------------- 5.5/25.1 MB 897.1 kB/s eta 0:00:22\n",
            "   -------- ------------------------------- 5.5/25.1 MB 897.1 kB/s eta 0:00:22\n",
            "   --------- ------------------------------ 5.8/25.1 MB 880.8 kB/s eta 0:00:22\n",
            "   --------- ------------------------------ 5.8/25.1 MB 880.8 kB/s eta 0:00:22\n",
            "   ---------- ----------------------------- 6.3/25.1 MB 897.4 kB/s eta 0:00:21\n",
            "   ---------- ----------------------------- 6.6/25.1 MB 915.1 kB/s eta 0:00:21\n",
            "   ---------- ----------------------------- 6.8/25.1 MB 932.1 kB/s eta 0:00:20\n",
            "   ----------- ---------------------------- 7.1/25.1 MB 948.2 kB/s eta 0:00:20\n",
            "   ----------- ---------------------------- 7.3/25.1 MB 957.6 kB/s eta 0:00:19\n",
            "   ------------ --------------------------- 7.6/25.1 MB 954.7 kB/s eta 0:00:19\n",
            "   ------------ --------------------------- 7.9/25.1 MB 959.6 kB/s eta 0:00:18\n",
            "   ------------ --------------------------- 7.9/25.1 MB 959.6 kB/s eta 0:00:18\n",
            "   ------------ --------------------------- 8.1/25.1 MB 946.0 kB/s eta 0:00:18\n",
            "   ------------- -------------------------- 8.4/25.1 MB 956.1 kB/s eta 0:00:18\n",
            "   -------------- ------------------------- 8.9/25.1 MB 985.2 kB/s eta 0:00:17\n",
            "   -------------- ------------------------- 9.2/25.1 MB 997.2 kB/s eta 0:00:16\n",
            "   --------------- ------------------------ 9.4/25.1 MB 1.0 MB/s eta 0:00:16\n",
            "   --------------- ------------------------ 9.7/25.1 MB 1.0 MB/s eta 0:00:16\n",
            "   --------------- ------------------------ 10.0/25.1 MB 1.0 MB/s eta 0:00:15\n",
            "   ---------------- ----------------------- 10.2/25.1 MB 1.0 MB/s eta 0:00:15\n",
            "   ---------------- ----------------------- 10.2/25.1 MB 1.0 MB/s eta 0:00:15\n",
            "   ----------------- ---------------------- 10.7/25.1 MB 1.0 MB/s eta 0:00:15\n",
            "   ----------------- ---------------------- 11.0/25.1 MB 1.0 MB/s eta 0:00:14\n",
            "   ------------------ --------------------- 11.5/25.1 MB 1.1 MB/s eta 0:00:13\n",
            "   ------------------ --------------------- 11.8/25.1 MB 1.1 MB/s eta 0:00:13\n",
            "   ------------------- -------------------- 12.1/25.1 MB 1.1 MB/s eta 0:00:13\n",
            "   ------------------- -------------------- 12.3/25.1 MB 1.1 MB/s eta 0:00:12\n",
            "   -------------------- ------------------- 12.6/25.1 MB 1.1 MB/s eta 0:00:12\n",
            "   -------------------- ------------------- 12.8/25.1 MB 1.1 MB/s eta 0:00:12\n",
            "   -------------------- ------------------- 12.8/25.1 MB 1.1 MB/s eta 0:00:12\n",
            "   --------------------- ------------------ 13.4/25.1 MB 1.1 MB/s eta 0:00:11\n",
            "   ---------------------- ----------------- 13.9/25.1 MB 1.1 MB/s eta 0:00:11\n",
            "   ---------------------- ----------------- 14.2/25.1 MB 1.1 MB/s eta 0:00:10\n",
            "   ----------------------- ---------------- 14.7/25.1 MB 1.1 MB/s eta 0:00:10\n",
            "   ----------------------- ---------------- 14.9/25.1 MB 1.1 MB/s eta 0:00:09\n",
            "   ------------------------ --------------- 15.2/25.1 MB 1.1 MB/s eta 0:00:09\n",
            "   ------------------------ --------------- 15.5/25.1 MB 1.1 MB/s eta 0:00:09\n",
            "   ------------------------- -------------- 15.7/25.1 MB 1.1 MB/s eta 0:00:09\n",
            "   ------------------------- -------------- 16.0/25.1 MB 1.1 MB/s eta 0:00:08\n",
            "   -------------------------- ------------- 16.5/25.1 MB 1.2 MB/s eta 0:00:08\n",
            "   -------------------------- ------------- 16.8/25.1 MB 1.2 MB/s eta 0:00:08\n",
            "   --------------------------- ------------ 17.0/25.1 MB 1.2 MB/s eta 0:00:07\n",
            "   --------------------------- ------------ 17.6/25.1 MB 1.2 MB/s eta 0:00:07\n",
            "   ---------------------------- ----------- 17.8/25.1 MB 1.2 MB/s eta 0:00:07\n",
            "   ---------------------------- ----------- 18.1/25.1 MB 1.2 MB/s eta 0:00:06\n",
            "   ----------------------------- ---------- 18.4/25.1 MB 1.2 MB/s eta 0:00:06\n",
            "   ----------------------------- ---------- 18.6/25.1 MB 1.2 MB/s eta 0:00:06\n",
            "   ------------------------------ --------- 18.9/25.1 MB 1.2 MB/s eta 0:00:06\n",
            "   ------------------------------ --------- 19.4/25.1 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------------- -------- 19.7/25.1 MB 1.2 MB/s eta 0:00:05\n",
            "   -------------------------------- ------- 20.2/25.1 MB 1.2 MB/s eta 0:00:05\n",
            "   -------------------------------- ------- 20.4/25.1 MB 1.2 MB/s eta 0:00:04\n",
            "   --------------------------------- ------ 21.0/25.1 MB 1.2 MB/s eta 0:00:04\n",
            "   --------------------------------- ------ 21.2/25.1 MB 1.2 MB/s eta 0:00:04\n",
            "   ---------------------------------- ----- 21.5/25.1 MB 1.2 MB/s eta 0:00:03\n",
            "   ---------------------------------- ----- 21.5/25.1 MB 1.2 MB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 22.0/25.1 MB 1.2 MB/s eta 0:00:03\n",
            "   ----------------------------------- ---- 22.5/25.1 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------------------------------ --- 23.1/25.1 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------------------------- -- 23.6/25.1 MB 1.3 MB/s eta 0:00:02\n",
            "   -------------------------------------- - 24.1/25.1 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 24.4/25.1 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.6/25.1 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------  24.9/25.1 MB 1.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 25.1/25.1 MB 1.3 MB/s eta 0:00:00\n",
            "Using cached simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
            "Using cached tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
            "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Using cached docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
            "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: promise\n",
            "  Building wheel for promise (setup.py): started\n",
            "  Building wheel for promise (setup.py): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21544 sha256=6bbbb6559735fb169ce69a99a2e04d973ef531917f02f99d437527cf24cb88f5\n",
            "  Stored in directory: c:\\users\\icardoso\\appdata\\local\\pip\\cache\\wheels\\90\\74\\b1\\9b54c896b8d9409e9268329d4d45ede8a8040abe91c8879932\n",
            "Successfully built promise\n",
            "Installing collected packages: dm-tree, zipp, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, fsspec, etils, docstring-parser, tensorflow-metadata, simple-parsing, tensorflow-datasets\n",
            "Successfully installed dm-tree-0.1.8 docstring-parser-0.16 etils-1.11.0 fsspec-2024.12.0 googleapis-common-protos-1.66.0 immutabledict-4.2.1 importlib_resources-6.4.5 promise-2.3 pyarrow-18.1.0 simple-parsing-0.1.6 tensorflow-datasets-4.9.7 tensorflow-metadata-1.16.1 toml-0.10.2 zipp-3.21.0\n",
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lMHwYXHXCar3"
      },
      "outputs": [],
      "source": [
        "# get data files\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Define file paths\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\"\n",
        "\n",
        "# URLs for the datasets\n",
        "train_url = \"https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\"\n",
        "test_url = \"https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\"\n",
        "\n",
        "# Function to download a file\n",
        "def download_file(url, file_path):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(file_path, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded {file_path}\")\n",
        "    else:\n",
        "        print(f\"Failed to download {file_path}. Status code: {response.status_code}\")\n",
        "\n",
        "# Download the datasets\n",
        "if not os.path.exists(train_file_path):\n",
        "    download_file(train_url, train_file_path)\n",
        "\n",
        "if not os.path.exists(test_file_path):\n",
        "    download_file(test_url, test_file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "outputs": [],
      "source": [
        "train_dataset = pd.read_csv('train-data.tsv',sep='\\t',header=None,names=['class', 'text'],encoding='utf-8')\n",
        "valid_dataset = pd.read_csv('valid-data.tsv',sep='\\t',header=None,names=['class', 'text'],encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 88584\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "def preprocess(df):\n",
        "    data = df.copy()\n",
        "    data.loc[data['class'] == 'ham','class'] = 0\n",
        "    data.loc[data['class'] == 'spam','class'] = 1\n",
        "    labels = data.pop('class').astype(np.float32)\n",
        "    new_data = []\n",
        "    for i in data['text']:\n",
        "      new_data.append(i)\n",
        "    return new_data, labels\n",
        "\n",
        "train_data, train_labels = preprocess(train_dataset)\n",
        "valid_data, valid_labels = preprocess(valid_dataset)\n",
        "\n",
        "# Calculating the maximum message length\n",
        "MAXLEN_train = max(train_data, key=len)\n",
        "MAXLEN_valid = max(valid_data, key=len)\n",
        "MAXLEN = max(len(MAXLEN_train.strip()), len(MAXLEN_valid.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Taking words indexes from preproc lib\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Function for encoding text message\n",
        "def encode_text(text):\n",
        "  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "# Func for prep text message for model\n",
        "def prep_to_mod(data_for_prep):\n",
        "    new_data = []\n",
        "    for i in data_for_prep:\n",
        "        new_data.append(encode_text(i))\n",
        "    return np.array(new_data)\n",
        "\n",
        "# Prep data for model\n",
        "new_train_data = prep_to_mod(train_data)\n",
        "new_valid_data = prep_to_mod(valid_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - acc: 0.8483 - loss: 0.4183 - val_acc: 0.9701 - val_loss: 0.1255\n",
            "Epoch 2/5\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 164ms/step - acc: 0.9650 - loss: 0.1357 - val_acc: 0.9797 - val_loss: 0.0812\n",
            "Epoch 3/5\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 156ms/step - acc: 0.9722 - loss: 0.0967 - val_acc: 0.9785 - val_loss: 0.0659\n",
            "Epoch 4/5\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 161ms/step - acc: 0.9850 - loss: 0.0567 - val_acc: 0.9821 - val_loss: 0.0620\n",
            "Epoch 5/5\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 150ms/step - acc: 0.9881 - loss: 0.0427 - val_acc: 0.9856 - val_loss: 0.0510\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x2da04a99f50>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fitting model\n",
        "model.fit(new_train_data, train_labels, epochs=5, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - acc: 0.9811 - loss: 0.0542\n",
            "[0.05789817497134209, 0.9820402264595032]\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(new_valid_data, valid_labels)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
            "[0.0011350012, 'ham']\n"
          ]
        }
      ],
      "source": [
        "# function to predict messages based on model\n",
        "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
        "def predict_message(pred_text):\n",
        "    encoded_text = encode_text(pred_text)\n",
        "    pred = np.zeros((1, MAXLEN))\n",
        "    pred[0] = encoded_text\n",
        "    result = model.predict(pred).flatten()[0]\n",
        "    label = \"\"\n",
        "    if result > 0.5:\n",
        "        label = \"spam\"\n",
        "    else:\n",
        "        label = \"ham\"\n",
        "    prediction = [result, label]\n",
        "    return (prediction)\n",
        "\n",
        "pred_text = \"how are you doing today?\"\n",
        "\n",
        "prediction = predict_message(pred_text)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "You passed the challenge. Great job!\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to test your function and model. Do not modify contents.\n",
        "def test_predictions():\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")\n",
        "\n",
        "test_predictions()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "fcc_sms_text_classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
